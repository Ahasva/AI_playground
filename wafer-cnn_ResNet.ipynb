{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fafce59-f24c-4f1e-81c8-a6907181ee1d",
   "metadata": {},
   "source": [
    "# WM811k Silicon Wafer Map Dataset Image\n",
    "\n",
    "WM811k Silicon Wafer Map Dataset with Defect Detection\n",
    "\n",
    "Dataset on => [Kaggle](https://www.kaggle.com/datasets/muhammedjunayed/wm811k-silicon-wafer-map-dataset-image/data)\n",
    "\n",
    "---\n",
    "<p>\n",
    "<b>TABLE OF CONTENTS</b>\n",
    "    \n",
    "    1. Setup\n",
    "    2. Data Preprocessing\n",
    "    3. Defining Convolutional Neural Network\n",
    "    4. Pipeline\n",
    "    5. Rerun *Stage 3* & (SWA), Fine-tuning\n",
    "    6. Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fedeb7a-1059-4960-97ad-b270260360e0",
   "metadata": {},
   "source": [
    "## Import & Device check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724a2e3-22c5-4c4e-975e-ab8ddbe8b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cuda\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "\n",
    "print(f\"Selected Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe1090-dc53-4947-b634-5b57b5f68bf6",
   "metadata": {},
   "source": [
    "## Learning Rate Finder\n",
    "\n",
    "<span style=\"color:yellow\">only use after `configure_stage` is in memory</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32eb272c-ebb1-4f7f-b237-e2012a0a39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_base_lr():\n",
    "    # 1) Coonfiguring Stage 1 (only head+fc unfrozen)\n",
    "    configure_stage(1)\n",
    "\n",
    "    # 2) Extracting clean copy of the param‐groups (strip out scheduler state)\n",
    "    fresh_groups = [\n",
    "        {\"params\": pg[\"params\"], \"lr\": pg[\"lr\"]}\n",
    "        for pg in optimizer.param_groups\n",
    "    ]\n",
    "\n",
    "    # 3) Build brand-new optimizer with exactly those groups, no scheduler\n",
    "    fresh_opt = torch.optim.Adam(fresh_groups, weight_decay=4e-4)\n",
    "\n",
    "    # 4) hand-over to LR-finder\n",
    "    lr_finder = LRFinder(model, fresh_opt, criterion, device=device)\n",
    "    lr_finder.range_test(train_loader, end_lr=1e-1, num_iter=100)\n",
    "    lr_finder.plot()     # inspect loss vs LR\n",
    "    lr_finder.reset()    # back to starting weights & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785b57d-a287-4aae-bd7a-0664d28699fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_base_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025fa22-ac73-4fae-93f4-0a466b80dcb8",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Select architecture (freeze as base) from `torchvision.models` like\n",
    "- `.resnet50(pretrained=True)`\n",
    "- `.resnet101(pretrained=True)`\n",
    "- `.resnet152(pretrained=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d9b4d-0edc-4bff-9b4d-f786108d1bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet152(pretrained=True)\n",
    "\n",
    "NUM_CLASSES = len(os.listdir(\"data/WM811k_Dataset\"))\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "model = model.to(device)\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"checkpoints/wafer_cnn_pretrained_backbone.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae42138-e730-419f-8f0e-07502e7514ae",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4150b07-5db9-43ca-9678-31c76229d7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE      = 32\n",
    "LR              = 1e-4\n",
    "NUM_EPOCHS      = 50\n",
    "IMG_SIZE        = 128\n",
    "DATA_ROOT       = \"data/WM811k_Dataset\"\n",
    "CHECKPOINT_ROOT = \"checkpoints\"\n",
    "\n",
    "os.makedirs(CHECKPOINT_ROOT, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b0f93e-a0f8-42e6-a1ea-8ec43a33ad30",
   "metadata": {},
   "source": [
    "## Data Preview\n",
    "\n",
    "<span style=\"color:yellow\">One random sample per class.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd2d0d9-fff9-4154-9629-b4233ca045cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = DATA_ROOT\n",
    "classes = sorted(d for d in os.listdir(root_dir)\n",
    "                 if os.path.isdir(os.path.join(root_dir, d)))\n",
    "\n",
    "samples = []\n",
    "for cls in classes:\n",
    "    cls_dir = os.path.join(root_dir, cls)\n",
    "    imgs = [f for f in os.listdir(cls_dir)\n",
    "            if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "    if not imgs: continue\n",
    "    img_path = os.path.join(cls_dir, random.choice(imgs))\n",
    "    samples.append((cls, img_path))\n",
    "\n",
    "n = len(samples)\n",
    "cols = 4\n",
    "rows = (n + cols - 1) // cols\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(cols*3, rows*3))\n",
    "axes = axes.flatten()\n",
    "for ax, (cls, path) in zip(axes, samples):\n",
    "    img = Image.open(path)\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(cls, fontsize=8)\n",
    "    ax.axis('off')\n",
    "for ax in axes[len(samples):]:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5353b-5f06-4f04-be9b-535d6f13f51b",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "- `tfm = transforms.Compose([...])` builds a chain of image transforms that will be applied to every sample.\n",
    "- `full_ds = datasets.ImageFolder(DATA_ROOT, transform=tfm)` wraps all images (organized in subfolders by class) into a single dataset that applies those transforms\n",
    "- `train_ds, val_ds = random_split(full_ds, [...])` splits dataset into 80/20 (train/validation) subsets\n",
    "- `DataLoader(...)` from `torch.utils.data` creates iterators over the train and validation subsets\n",
    "\n",
    "Outcome:\n",
    "- `train_loader` and `val_loader` will yield augmented, normalized batches ready for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031292b7-6a37-43cb-8ab0-a0b00b295db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),      \n",
    "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
    "    transforms.RandomErasing(p=0.25, scale=(0.02,0.10)),\n",
    "    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([.485,.456,.406],[.229,.224,.225]),\n",
    "])\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(\n",
    "        p=0.25, scale=(0.02, 0.10), ratio=(0.3, 3.3), value='random'\n",
    "    ),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    ),\n",
    "])\n",
    "\n",
    "\n",
    "full_ds = datasets.ImageFolder(DATA_ROOT, transform=tfm)\n",
    "n = len(full_ds)\n",
    "train_ds, val_ds = random_split(full_ds, [int(0.8*n), n - int(0.8*n)])\n",
    "\n",
    "pin = True if device.type == \"cuda\" else False\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=pin\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=pin\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f8e14-339e-439e-842d-0a7709f92001",
   "metadata": {},
   "source": [
    "## Loss function & `configure_stage`\n",
    "\n",
    "1) **Loss function**\n",
    "- Equations and details on *CrossEntropyLoss* are explained here: [official PyTorch documentation](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "\n",
    "\n",
    "2) **Helper function** (`configure_stage`)\n",
    "- `configure_stage` is helper function to set up each fine-tuning *stage* (see below)#\n",
    "- Core elements of helper function:\n",
    "  - <span style=\"color:magenta\">Freeze/Unfreeze</span>\n",
    "    - Stage 1: only training of classifier head (layer4+fc);\n",
    "    - Stage 2: unfreezing of layer3;\n",
    "    - Stage 3: unfreezing of all layers; BatchNorm layers are put into eval mode (freezing their running stats);\n",
    "  - <span style=\"color:magenta\">Parameter groups & learning rates (LR)</span>\n",
    "    - Building `optimizer` groups with different LRs for head, mid, and earlier layers, scaled by the `head_mult`, `layer3_mult`, etc.;\n",
    "    - Picking a list of `max_lr` values for OneCycle scheduling;\n",
    "  - <span style=\"color:magenta\">Optimizer & Schedulers</span>\n",
    "    - Creates an Adam optimizer with weight decay;\n",
    "    - Wraps it in a OneCycleLR scheduler (with % warm-up, `div_factor` initial slope, `final_div_factor` tail);\n",
    "    - Wraps the model in a *Stochastic Weight Averaging* (SWA) wrapper (`swa_model`) and an SWA LR scheduler (`swa_scheduler`), for starting averaging later:\n",
    "      - instead of taking the final model’s weights at the end of training, a running average of the weights over multiple points (typically late in training) is maintained and that averaged model is used at test time. It tends to sit in wider, flatter regions of the loss landscape and gives better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a4294-b1c9-44d4-85d4-b0d89f0c2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "def configure_stage(stage:int,\n",
    "                    pct_start=None,\n",
    "                    div_factor=None,\n",
    "                    final_div_factor=None,\n",
    "                    layer2_mult=None,\n",
    "                    layer3_mult=None,\n",
    "                    head_mult=None,\n",
    "                    swa_frac=None):\n",
    "    global optimizer, scheduler, swa_model, swa_scheduler\n",
    "\n",
    "    # 1) freeze/unfreeze\n",
    "    for name, p in model.named_parameters():\n",
    "        if stage == 1:\n",
    "            p.requires_grad = name.startswith((\"layer4\",\"fc\"))\n",
    "        elif stage == 2:\n",
    "            p.requires_grad = name.startswith((\"layer3\",\"layer4\",\"fc\"))\n",
    "        else:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    # 2) param‐groups + max_lr\n",
    "    if stage == 1:\n",
    "        hm = head_mult or 2.0\n",
    "        head = list(model.layer4.parameters()) + list(model.fc.parameters())\n",
    "        groups, max_lr = [{\"params\": head, \"lr\": LR*hm}], [LR*hm]\n",
    "\n",
    "    elif stage == 2:\n",
    "        hm = head_mult or 1.75\n",
    "        mm = layer3_mult or 1.05\n",
    "        head = list(model.layer4.parameters()) + list(model.fc.parameters())\n",
    "        mid  = list(model.layer3.parameters())\n",
    "        groups = [\n",
    "            {\"params\": head, \"lr\": LR*hm},\n",
    "            {\"params\": mid,  \"lr\": LR*mm},\n",
    "        ]\n",
    "        max_lr = [LR*hm, LR*mm]\n",
    "\n",
    "    else:\n",
    "        l2 = layer2_mult or 0.3\n",
    "        l3 = layer3_mult or 0.6\n",
    "        hm = head_mult or 2.5\n",
    "        groups = [\n",
    "            {\"params\": model.layer1.parameters(),\"lr\":LR*0.1},\n",
    "            {\"params\": model.layer2.parameters(),\"lr\":LR*l2},\n",
    "            {\"params\": model.layer3.parameters(),\"lr\":LR*l3},\n",
    "            {\"params\": model.layer4.parameters(),\"lr\":LR*1.0},\n",
    "            {\"params\": model.fc.parameters(),\"lr\":LR*hm},\n",
    "        ]\n",
    "        max_lr = [LR*x for x in (0.1, l2, l3, 1.0, hm)]\n",
    "        # freezes running stats in BN\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d): m.eval()\n",
    "\n",
    "    # 3) optimizer + OneCycle + SWA\n",
    "    optimizer = optim.Adam(groups, weight_decay=4e-4)\n",
    "    ps = pct_start or 0.35\n",
    "    df = div_factor or 10\n",
    "    fdf = final_div_factor or 8\n",
    "    \n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=max_lr,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=NUM_EPOCHS,\n",
    "        pct_start=ps,\n",
    "        div_factor=df,\n",
    "        final_div_factor=fdf,\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    swa_model = AveragedModel(model)\n",
    "    swa_scheduler = SWALR(\n",
    "        optimizer,\n",
    "        swa_lr=LR * (swa_frac or 0.2)\n",
    "    )\n",
    "\n",
    "    # 4) summary\n",
    "    if stage == 1:\n",
    "        print(f\"→ Stage 1 configured (head×{hm:.2f})\")\n",
    "    elif stage == 2:\n",
    "        print(f\"→ Stage 2 configured (head×{hm:.2f}, mid×{mm:.2f}, \"\n",
    "              f\"pct_start={ps:.2f}, div_factor={df}, final_df={fdf})\")\n",
    "    else:\n",
    "        print(f\"→ Stage 3 configured (l2×{l2:.2f}, l3×{l3:.2f}, head×{hm:.2f}, \"\n",
    "              f\"pct_start={ps:.2f}, div_factor={df}, final_df={fdf}, swa_frac={swa_frac or 0.2:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f1a793-7595-44b8-971c-00f8b783765d",
   "metadata": {},
   "source": [
    "## `run_stage` helper with `model.train()` & `model.eval()`\n",
    "\n",
    "`run_stage` is a helper function that encapsulates a full *stage* of training with one-cycle scheduling, early stopping, and *Stochastic Weight Averaging* (SWA).\n",
    "<p></p>\n",
    "\n",
    "**Basic Structure:**\n",
    "\n",
    "1) <span style=\"color:magenta\">Stage config</span>\n",
    "2) <span style=\"color:magenta\">Epoch loop</span>\n",
    "   - Train (puts model in train mode, iterates over `train_loader`, computes cross-entropy, backpropagation, stepping the optimizer, advancing LR scheduler, tracking of last batch's loss (`trL`) and cumulative training accuracy (`trA`))\n",
    "   - Validate (switches to eval mode, runs `val_loader` without gradients, accumulating total validation loss (`vL`) and accuracy (`vA`))\n",
    "   - Logging & history\n",
    "3) <span style=\"color:magenta\">Early stopping & checkpointing</span>\n",
    "   - If current `vL` is best, resets the early-stop counter and saves a “best” checkpoint\n",
    "   - Otherwise increments a “stagnation” counter, and breaks out early, if it hits 10 epochs without improvement\n",
    "4) <span style=\"color:magenta\">Stochastic Weight Averaging</span>\n",
    "   - After 75% of epochs, each epoch also updates the SWA model’s weights and its SWA LR scheduler\n",
    "5) <span style=\"color:magenta\">Final checkpoint save</span> (regardless of early stop) & <span style=\"color:magenta\">return</span> of lists of train loss, train acc, val loss and val acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f298c9b9-3712-4792-bff2-29c5a205232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stage(stage: int, **cfg_overrides):\n",
    "    configure_stage(stage, **cfg_overrides)\n",
    "    EPOCH_SWA = int(NUM_EPOCHS * 0.75)\n",
    "    best = float('inf')\n",
    "    v_streak = 0\n",
    "    tl = []\n",
    "    vl = []\n",
    "    ta = []\n",
    "    va = []\n",
    "    \n",
    "    for ep in range(1, NUM_EPOCHS + 1):\n",
    "        # ------- Training -------\n",
    "        model.train()\n",
    "        tot = 0\n",
    "        cor = 0\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(out,y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = out.argmax(1)\n",
    "            tot += y.size(0)\n",
    "            cor += (preds == y).sum().item()\n",
    "            scheduler.step()\n",
    "        trL = loss.item()\n",
    "        trA = cor/tot\n",
    "        # ------- Validation -------\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot = 0\n",
    "            cor = 0\n",
    "            vloss = 0\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "                out = model(x)\n",
    "                vloss += nn.CrossEntropyLoss(label_smoothing=0.1)(out,y).item()*y.size(0)\n",
    "                preds = out.argmax(1)\n",
    "                tot += y.size(0)\n",
    "                cor += (preds==y).sum().item()\n",
    "            vL = vloss/tot\n",
    "            vA = cor/tot\n",
    "        # ------- Record & Print -------\n",
    "        tl.append(trL)\n",
    "        vl.append(vL)\n",
    "        ta.append(trA)\n",
    "        va.append(vA)\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"S{stage} E{ep:02} | LR {lr:.4e}; Tr {trL:.3f} / {trA:.3f}; V {vL:.3f} / {vA:.3f}\")\n",
    "\n",
    "        # ------- Early‐stop & Checkpoint -------\n",
    "        if vL < best:\n",
    "            best, v_streak = vL, 0\n",
    "            torch.save(model.state_dict(),f\"{CHECKPOINT_ROOT}/wafer_cnn_stage{stage}.pt\")\n",
    "        else:\n",
    "            v_streak += 1\n",
    "            if v_streak >= 10:\n",
    "                print(f\"→ early stop triggered after {v_streak} stagnant epochs\")\n",
    "                break\n",
    "\n",
    "        # ------- SWA step -------\n",
    "        if ep >= EPOCH_SWA:\n",
    "            swa_model.update_parameters(model)\n",
    "            swa_scheduler.step()\n",
    "\n",
    "    torch.save(model.state_dict(),f\"{CHECKPOINT_ROOT}/wafer_cnn_stage{stage}_final.pt\")\n",
    "    return tl,vl,ta,va"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9f473-3570-465e-891d-aa793e7ec1c0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Stage 1</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5b3596-86be-4210-b72c-8d2fac8d9991",
   "metadata": {},
   "source": [
    "### General briefing on params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d37482-ea69-4edb-8da3-714ae8c74d6c",
   "metadata": {},
   "source": [
    "1) **pct_start** (warm-up fraction)\n",
    "   - Higher: spend more of the one-cycle schedule slowly ramping from low LR up to the peak (hence, model “warms in” more gently)\n",
    "   - Lower: quicker ramp to peak LR, more time spent decaying; warms up fast but risks instability early.<p>\n",
    "2) **div_factor** (initial LR divisor)\n",
    "   - Higher: starting `LR = max_lr / div_factor` is smaller, so climbing up more slowly at first (safer but slower to learn)\n",
    "   - Lower: starting closer to max_lr, hence, a sharper climb (faster initial learning, but may blow past the optimum)<p>\n",
    "3) **final_div_factor** (final LR divisor)\n",
    "   - Higher: your final `LR = max_lr / final_div_factor` is smaller, so the tail of the cycle decays to a very low LR (gentler “cool-down,” less drift)\n",
    "   - Lower: final LR sits higher, hence, end with more aggressive steps (can adapt more late in training, but may destabilize)<p>\n",
    "4) **head_mult** (LR multiplier on the new head)\n",
    "   - Higher: the classifier layers (layer4 + fc) learn faster; good if they’re sorely undertrained, but risks overfitting or noisy updates\n",
    "   - Lower: slows the head’s LR relative to the backbone—safer, if head is already close, but may be too timid to learn new classes<p>\n",
    "5) **swa_frac** (SWA LR as a fraction of base LR)\n",
    "   - Higher: SWA updates use a larger LR (more exploratory averaging, can converge faster to flatter minima but can oscillate)\n",
    "   - Lower: smaller SWA LR (more conservative averaging, smoother but slower to incorporate new snapshots)\n",
    "\n",
    "<p></p>\n",
    "\n",
    "**In practice:**\n",
    "    \n",
    " * bumping *pct_start* up if early unstable spikes,\n",
    " * raising *div_factor* for a gentler climb, if val-loss jumps,\n",
    " * tweaking *final_div_factor* to land at a sweet low LR at the end,\n",
    " * nudging *head_mult* up/down to let the head learn more or less aggressively,\n",
    " * adjusting *swa_frac* to control how strongly your SWA ensemble moves in late training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e7e6c-75eb-4949-a8c7-a50b447d415d",
   "metadata": {},
   "source": [
    "### Stage 1 <span style=\"color:red\">customized</span> run + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab19dc8-d8fd-4a15-bdad-de8073431394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Re-instantiation of “same” model architecture\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES) \n",
    "\n",
    "# 2) Loading of saved backbone + fresh head\n",
    "model.load_state_dict(torch.load(\n",
    "    f\"{CHECKPOINT_ROOT}/wafer_cnn_pretrained_backbone.pt\"),\n",
    "    strict=False\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3) Running Stage 1 with custom overrides\n",
    "tl1, vl1, ta1, va1 = run_stage(\n",
    "    1,\n",
    "    # Param tuning for getting best val loss & val acc\n",
    "    pct_start        = 0.30,  # longer warm-up\n",
    "    div_factor       = 12,    # slower climb\n",
    "    final_div_factor = 10,    # gentler decay\n",
    "    head_mult        = 2.25,  # head LR = LR * 2.25\n",
    "    swa_frac         = 0.20,  # SWA LR = 20% of base\n",
    ")\n",
    "\n",
    "# 4) Plotting Stage 1 curves\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl1, label='train loss')\n",
    "plt.plot(vl1,label='val loss')\n",
    "plt.title(\"Stage 1 Loss\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta1, label='trainacc')\n",
    "plt.plot(va1, label='val acc')\n",
    "plt.title(\"Stage 1 Accuracy\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c4105-ae55-44e9-8810-13be8b313e0f",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Stage 2</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9ba3c4-21d5-4d99-9df0-868be0b44e5a",
   "metadata": {},
   "source": [
    "### Stage 2 <span style=\"color:red\">customized</span> run + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe1321c-ec47-4333-82de-2881b9cc7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.A) Re‐instantiating ResNet-152 architecture\n",
    "# Needed only, if starting from a fresh Python session or if entire reset of model graph is intended.\n",
    "#model = models.resnet152(pretrained=False)\n",
    "#model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# 0.B) Loading saved pretrained backbone + fresh head\n",
    "#model.load_state_dict(\n",
    "#    torch.load(\"checkpoints/wafer_cnn_pretrained_backbone.pt\"),\n",
    "#    strict=False\n",
    "#)\n",
    "\n",
    "# 1) Loading Stage-1 weights\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"{CHECKPOINT_ROOT}/wafer_cnn_stage1_final.pt\",\n",
    "        map_location=device\n",
    "    ),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "# 2) Running Stage 2 with tuned hyper-params\n",
    "tl2, vl2, ta2, va2 = run_stage(\n",
    "    2,\n",
    "    pct_start        = 0.35,\n",
    "    div_factor       = 10,\n",
    "    final_div_factor = 8,\n",
    "    head_mult        = 1.75,\n",
    "    layer3_mult      = 1.05,\n",
    "    swa_frac         = 0.20,\n",
    ")\n",
    "\n",
    "# 3) Plotting Stage 2 curves\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl2, label='train loss')\n",
    "plt.plot(vl2, label='val loss')\n",
    "plt.title(\"Stage 2 Loss\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta2, label='train acc')\n",
    "plt.plot(va2, label='val acc')\n",
    "plt.title(\"Stage 2 Accuracy\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669a897-b1c0-4d72-af72-23624845b79b",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\">Stage 3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b92c9-768e-4333-8d77-a64618773efc",
   "metadata": {},
   "source": [
    "### Stage 3 <span style=\"color:red\">customized</span> run + plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f1f25-c526-40c6-b968-42e7ffa2e35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Loading best Stage-2 weights\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"{CHECKPOINT_ROOT}/wafer_cnn_stage2_final.pt\",\n",
    "        map_location=device\n",
    "    ),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "# 2) One-pass sanity check on the validation set\n",
    "model.eval()\n",
    "tot = 0\n",
    "cor = 0\n",
    "vloss = 0\n",
    "ce = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        logits = model(xb)\n",
    "        vloss += ce(logits, yb).item() * yb.size(0)\n",
    "        cor += (logits.argmax(1) == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "\n",
    "vL = vloss/tot\n",
    "vA = cor/tot\n",
    "print(f\"Before Stage-3\\tVal Loss {vL:.3f} / Val Acc {vA:.3f}\\n\")\n",
    "\n",
    "# 3) Run Stage 3 with new hyper-params\n",
    "tl3, vl3, ta3, va3 = run_stage(\n",
    "    3,\n",
    "    pct_start        = 0.45, # 0.45\n",
    "    div_factor       = 5,\n",
    "    final_div_factor = 25, # 20\n",
    "    layer2_mult      = 0.30,\n",
    "    layer3_mult      = 0.65, # best: 0.65\n",
    "    head_mult        = 2.80, # 2.80\n",
    "    swa_frac         = 0.30, # 0.25\n",
    ")\n",
    "\n",
    "# 4) Plot curves\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl3, label='train loss')\n",
    "plt.plot(vl3, label='val loss')\n",
    "plt.title(\"Stage 3 Loss\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta3, label='train acc')\n",
    "plt.plot(va3, label='val acc')\n",
    "plt.title(\"Stage 3 Accuracy\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2e37b-d06d-4a7a-b0d4-fbc943d74c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        f\"{CHECKPOINT_ROOT}/wafer_cnn_stage2_final.pt\"\n",
    "    ),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "tl3, vl3, ta3, va3 = run_stage(\n",
    "    3,\n",
    "    pct_start        = 0.35,   # slower warm-up\n",
    "    div_factor       = 8,      # tad gentler initial climb\n",
    "    final_div_factor = 20,     # longer/finer tail\n",
    "    layer2_mult      = 0.30,\n",
    "    layer3_mult      = 0.60,\n",
    "    head_mult        = 3.0,    # cut head LR in half\n",
    "    swa_frac         = 0.25,   # back to stable 25 %\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl3,label='train loss')\n",
    "plt.plot(vl3,label='val loss')\n",
    "plt.title(\"Stage 3 Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta3,label='train acc')\n",
    "plt.plot(va3,label='val acc')\n",
    "plt.title(\"Stage 3 Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cc087d-2d83-4e97-93da-a0e4b24cb1a8",
   "metadata": {},
   "source": [
    "### Cell A: Building & running of Stage-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125a731-2550-45f8-9571-d42a5a792815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) ------- Building model as in Stage-2 -------\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.30),\n",
    "    nn.Linear(512, NUM_CLASSES),\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 2) ------- Loading best Stage-2 weights -------\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CHECKPOINT_ROOT}/wafer_cnn_stage2_final.pt\",\n",
    "               map_location=device),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "NUM_EPOCHS = 30 \n",
    "\n",
    "# 3) ------- Running Stage-3 with tweaked schedule -------\n",
    "tl3, vl3, ta3, va3 = run_stage(\n",
    "    3,\n",
    "    pct_start        = 0.30, # best: 0.35\n",
    "    div_factor       = 6,    # best: 8\n",
    "    final_div_factor = 30,   # best: 40, 20\n",
    "    layer2_mult      = 0.30, # best: 0.30\n",
    "    layer3_mult      = 0.60, # best: 0.60\n",
    "    head_mult        = 2.5,  # best: 3.0\n",
    "    swa_frac         = 0.25, # best: 0.25\n",
    ")\n",
    "\n",
    "# 4) ------- Plotting -------\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl3,label='train loss')\n",
    "plt.plot(vl3,label='val loss')\n",
    "plt.title(\"Stage 3 Loss\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta3,label='train acc')\n",
    "plt.plot(va3,label='val acc')\n",
    "plt.title(\"Stage 3 Accuracy\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ccb0c-1bcd-491d-883b-70eeadc006a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the weight file written at the \"val loss\" minimum\n",
    "best_s3 = torch.load(\n",
    "    f\"{CHECKPOINT_ROOT}/wafer_cnn_stage3.pt\",\n",
    "    map_location=device\n",
    ")\n",
    "\n",
    "# Stash copy with an explicit name\n",
    "torch.save(\n",
    "    best_s3,\n",
    "    f\"{CHECKPOINT_ROOT}/wafer_cnn_stage3_best983.pt\"\n",
    ")\n",
    "\n",
    "print(\"✓ 98.3 % checkpoint saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66fe52-1b88-4b30-b80c-d7ee74e7330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Re-instantiation of neural network\n",
    "swa_model = AveragedModel(model)     # ResNet-152 + 2-layer head\n",
    "swa_model.load_state_dict(best_s3, strict=False)\n",
    "swa_model = swa_model.to(device)\n",
    "\n",
    "# Refresh BatchNorm statistics\n",
    "update_bn(train_loader, swa_model, device=device)\n",
    "\n",
    "torch.save(\n",
    "    swa_model.state_dict(),\n",
    "    f\"{CHECKPOINT_ROOT}/wafer_cnn_stage3_swa_best983.pt\"\n",
    ")\n",
    "print(\"✓ SWA version saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7138646-e5be-441a-a4e5-54485f2ae6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_eval(net):\n",
    "    net.eval(); tot=cor=loss=0; ce = nn.CrossEntropyLoss(label_smoothing=0.02)\n",
    "    with torch.no_grad():\n",
    "        for xb,yb in val_loader:\n",
    "            xb,yb = xb.to(device), yb.to(device)\n",
    "            logits = net(xb)\n",
    "            loss  += ce(logits,yb).item()*yb.size(0)\n",
    "            cor   += (logits.argmax(1)==yb).sum().item()\n",
    "            tot   += yb.size(0)\n",
    "    return loss/tot, cor/tot\n",
    "\n",
    "for tag, ckpt in [(\"Best\", best_s3),\n",
    "                  (\"SWA \", torch.load(f\"{CHECKPOINT_ROOT}/wafer_cnn_stage3_swa_best983.pt\",\n",
    "                                      map_location=device))]:\n",
    "    test_model = models.resnet152(pretrained=False)\n",
    "    test_model.fc = model.fc  # reuse the 2-layer head definition\n",
    "    test_model.load_state_dict(ckpt, strict=False)\n",
    "    test_model = test_model.to(device)\n",
    "    vL, vA = quick_eval(test_model)\n",
    "    print(f\"{tag} ➜ val loss {vL:.3f} · val acc {vA:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3129117-2233-4acf-bf08-d25eceb89dfe",
   "metadata": {},
   "source": [
    "\n",
    "### One-shot Stage-3 run\n",
    "- MixUp (α=0.10) + CE-LS (ε=0.01)\n",
    "- MLP-head dropout 0.15\n",
    "- logs *train loss / train acc* on soft labels & a “clean” hard-label train-batch accuracy every epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00500de-6bfb-43e9-a1fe-c4b2cc0bebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- user flags --------------------------------------------------------\n",
    "use_mixup  = True    # set False to disable\n",
    "use_cutmix = False   # set True to switch from MixUp → CutMix\n",
    "alpha_mu   = 0.05    # β-dist α; best: 0.10, 0.20\n",
    "label_eps  = 0.005   # label-smoothing ε; best: 0.01, 0.02\n",
    "drop_p     = 0.10    # dropout in the 2-layer head; best: 0.15, 0.20\n",
    "NUM_EPOCHS = 60\n",
    "early_pat  = 20      # best: 12\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# 1) model with new dropout\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(drop_p),\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CHECKPOINT_ROOT}/wafer_cnn_stage2_final.pt\", map_location=\"cpu\"),\n",
    "    strict=False\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "# 2) Stage-3 LR / optim setup\n",
    "configure_stage(\n",
    "    3,\n",
    "    pct_start        = 0.30,\n",
    "    div_factor       = 6,\n",
    "    final_div_factor = 60,\n",
    "    layer2_mult      = 0.30,\n",
    "    layer3_mult      = 0.60,\n",
    "    head_mult        = 2.50,\n",
    "    swa_frac         = 0.25,\n",
    ")\n",
    "\n",
    "# 3) helpers\n",
    "def ce_ls(logits, targets):\n",
    "    \"\"\"Cross-entropy with label-smoothing that works for hard or soft labels.\"\"\"\n",
    "    logp = logits.log_softmax(1)\n",
    "    if targets.dtype == torch.long:            # hard labels\n",
    "        nll = F.nll_loss(logp, targets, reduction='none')\n",
    "        return (1 - label_eps) * nll.mean() + label_eps * (-logp.mean(1)).mean()\n",
    "    else:                                      # soft labels (MixUp / CutMix)\n",
    "        return (-targets * logp).sum(1).mean()\n",
    "\n",
    "def mix_or_cut(x, y):\n",
    "    lam = np.random.beta(alpha_mu, alpha_mu)\n",
    "    idx = torch.randperm(x.size(0), device=x.device)\n",
    "    if use_cutmix and not use_mixup:\n",
    "        H, W = x.shape[2:]\n",
    "        cx = np.random.randint(W)\n",
    "        cy = np.random.randint(H)\n",
    "        w = int(W * np.sqrt(1 - lam))\n",
    "        h = int(H * np.sqrt(1 - lam))\n",
    "        x[:, :, max(cy-h//2,0):min(cy+h//2,H),\n",
    "              max(cx-w//2,0):min(cx+w//2,W)] = \\\n",
    "            x[idx, :, max(cy-h//2,0):min(cy+h//2,H),\n",
    "                     max(cx-w//2,0):min(cx+w//2,W)]\n",
    "    else:                                      # MixUp\n",
    "        x = lam * x + (1 - lam) * x[idx]\n",
    "        \n",
    "    y_soft = lam * F.one_hot(y, NUM_CLASSES) + (1 - lam) * F.one_hot(y[idx], NUM_CLASSES)\n",
    "    return x, y_soft\n",
    "\n",
    "# 4) training loop \n",
    "tl = []\n",
    "vl = []\n",
    "ta = []\n",
    "va = []\n",
    "best_vA = 0.0\n",
    "stagn = 0\n",
    "\n",
    "for ep in range(1, NUM_EPOCHS + 1):\n",
    "    # ------- train -------\n",
    "    model.train()\n",
    "    tot = 0\n",
    "    cor = 0\n",
    "    tloss = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        xb, tgt = mix_or_cut(xb, yb) if (use_mixup or use_cutmix) else (xb, yb)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = ce_ls(out, tgt)\n",
    "        loss.backward(); optimizer.step(); scheduler.step()\n",
    "        tloss += loss.item() * yb.size(0)\n",
    "        cor += (out.argmax(1) == yb).sum().item()   # accuracy vs hard labels\n",
    "        tot += yb.size(0)\n",
    "    trL = tloss / tot\n",
    "    trA = cor / tot\n",
    "\n",
    "    # ------- validate -------\n",
    "    model.eval()\n",
    "    tot = 0\n",
    "    cor = 0\n",
    "    vloss = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb)\n",
    "            vloss += ce_ls(out, yb).item() * yb.size(0)\n",
    "            cor += (out.argmax(1) == yb).sum().item()\n",
    "            tot += yb.size(0)\n",
    "    vL = vloss / tot\n",
    "    vA = cor / tot\n",
    "\n",
    "    # ------- one clean train-batch accuracy (sanity-check) -------\n",
    "    with torch.no_grad():\n",
    "        xb_c, yb_c = next(iter(train_loader))\n",
    "        xb_c = xb_c.to(device)\n",
    "        yb_c = yb_c.to(device)\n",
    "        clean_acc = (model(xb_c).argmax(1) == yb_c).float().mean().item()\n",
    "\n",
    "    # ------- logging & early-stop -------\n",
    "    tl.append(trL)\n",
    "    vl.append(vL)\n",
    "    ta.append(trA)\n",
    "    va.append(vA)\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    print(f\"S3 E{ep:02} | LR {lr:.3e}; Tr {trL:.3f}/{trA:.3f} (clean {clean_acc:.3f}); V {vL:.3f}/{vA:.3f}\")\n",
    "\n",
    "    if vA > best_vA:\n",
    "        best_vA = vA\n",
    "        stagn = 0\n",
    "    else:\n",
    "        stagn += 1\n",
    "    if stagn >= early_pat:\n",
    "        print(f\"⟲ early stop after {stagn} stagnant epochs\")\n",
    "        break\n",
    "\n",
    "# 5) curves\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tl, label='train')\n",
    "plt.plot(vl, label='val')\n",
    "plt.title('Loss')\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ta, label='train')\n",
    "plt.plot(va, label='val')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad0bb83-86d2-49aa-b07b-7c2570fd40ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Rebuilding same model architecture\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.15), # the dropout used in MixUp run\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ").to(device)\n",
    "\n",
    "# 2) Loading checkpoint, saved during training at E08 (see above)\n",
    "model.load_state_dict(\n",
    "    torch.load(f\"{CHECKPOINT_ROOT}/wafer_cnn_stage3.pt\",\n",
    "               map_location=\"cpu\"),\n",
    "    strict=False\n",
    ")\n",
    "\n",
    "# 3) Re-save\n",
    "torch.save(model.state_dict(),\n",
    "           f\"{CHECKPOINT_ROOT}/stage3_mixup_E08.pt\")\n",
    "print(\"=> stage3_mixup_E08.pt saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776d9bfc-e077-49d6-92cd-0a0812726e4f",
   "metadata": {},
   "source": [
    "## Cell A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c79a36f-8432-4c75-a629-1e1f6f74e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_built()\n",
    "                      else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CKPT_PATH = f\"{CHECKPOINT_ROOT}/stage3_mixup_E08.pt\"   # MixUp ckpt (10 classes)\n",
    "NUM_CLASSES = 9\n",
    "\n",
    "# 1) Building 10-class architecture that ckpt expects\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.10),\n",
    "    nn.Linear(512, 10) # 10-class head\n",
    ").to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(CKPT_PATH, map_location=\"cpu\"), strict=True)\n",
    "print(\"✓ backbone + 10-class head loaded\")\n",
    "\n",
    "model.fc[3] = nn.Linear(512, NUM_CLASSES)\n",
    "torch.nn.init.kaiming_normal_(model.fc[3].weight)\n",
    "model.fc[3].bias.data.zero_()\n",
    "print(\"→ swapped in fresh 9-class head\")\n",
    "\n",
    "# Pushing every param to the target device)\n",
    "model = model.to(device) # key without weights stay on CPU\n",
    "\n",
    "# Freezing backbone for quick head warm-up\n",
    "for n,p in model.named_parameters():\n",
    "    p.requires_grad = n.startswith(\"fc.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59aeecf-cf1d-4173-9a85-66edcf9ac2cd",
   "metadata": {},
   "source": [
    "## Cell B\n",
    "\n",
    "5-epoch head warm-up (train + val metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8c7d4-263a-453f-9a22-c08ff71f418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_built() else\n",
    "    \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "HEAD_LR = 5e-4 # LR only for the new fc.3 layer\n",
    "EPOCHS_H = 5   # quick bootstrap\n",
    "BATCHES = len(train_loader)\n",
    "\n",
    "# Only params that were left unfrozen (fc.3) are trainable\n",
    "opt = Adam([p for p in model.parameters() if p.requires_grad], lr=HEAD_LR)\n",
    "sched = OneCycleLR(opt, max_lr=HEAD_LR,\n",
    "                   steps_per_epoch=BATCHES, epochs=EPOCHS_H)\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "for ep in range(1, EPOCHS_H + 1):\n",
    "    # ------- Training -------\n",
    "    model.train()\n",
    "    tr_tot = 0\n",
    "    tr_cor = 0\n",
    "    tr_loss = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = ce(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        tr_loss += loss.item() * yb.size(0)\n",
    "        tr_cor += (out.argmax(1) == yb).sum().item()\n",
    "        tr_tot += yb.size(0)\n",
    "\n",
    "    trA = tr_cor / tr_tot\n",
    "    trL = tr_loss / tr_tot\n",
    "\n",
    "    # ------- Quick validation -------\n",
    "    model.eval()\n",
    "    vl_tot = 0\n",
    "    vl_cor = 0\n",
    "    vl_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb)\n",
    "            vl_loss += ce(out, yb).item() * yb.size(0)\n",
    "            vl_cor += (out.argmax(1) == yb).sum().item()\n",
    "            vl_tot += yb.size(0)\n",
    "\n",
    "    vlA = vl_cor / vl_tot\n",
    "    vlL = vl_loss / vl_tot\n",
    "\n",
    "    # ------- Logging -------\n",
    "    print(f\"warm-up E{ep:02} | Tr {trL:.3f}/{trA:.3f}; V {vlL:.3f}/{vlA:.3f}\")\n",
    "\n",
    "print(\"✓ head warm-up complete — ready to unfreeze backbone and launch Stage-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561c762-d487-43e9-8f98-4ceac05fa30d",
   "metadata": {},
   "source": [
    "## Cell C\n",
    "\n",
    "full-network fine-tune + SWA + metric logging + plots (with MPS fix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ee6c72-3d0c-4740-b64c-73cc48f7631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable CPU fallback for missing MPS ops (must come before torch import)\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# ------- Device -------\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_built() else\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "\n",
    "# ------- 0) Preparing metric lists -------\n",
    "tr_losses = []\n",
    "tr_accs = []\n",
    "vl_losses = []\n",
    "vl_accs = []\n",
    "\n",
    "# ------- 1) Unfreezing everything -------\n",
    "for p in model.parameters(): \n",
    "    p.requires_grad = True\n",
    "\n",
    "# ------- 2) Tiered learning-rates -------\n",
    "LR_BASE = 3e-5\n",
    "opt = Adam([\n",
    "    {\"params\": model.layer1.parameters(), \"lr\": LR_BASE * 0.2},\n",
    "    {\"params\": model.layer2.parameters(), \"lr\": LR_BASE * 0.4},\n",
    "    {\"params\": model.layer3.parameters(), \"lr\": LR_BASE * 0.8},\n",
    "    {\"params\": model.layer4.parameters(), \"lr\": LR_BASE * 1.0},\n",
    "    {\"params\": model.fc.parameters(),     \"lr\": LR_BASE * 4.0},\n",
    "])\n",
    "EPOCHS_F = 10\n",
    "BATCHES = len(train_loader)\n",
    "sched = OneCycleLR(\n",
    "    opt,\n",
    "    max_lr=[g[\"lr\"] for g in opt.param_groups],\n",
    "    epochs=EPOCHS_F,\n",
    "    steps_per_epoch=BATCHES,\n",
    ")\n",
    "\n",
    "ce = nn.CrossEntropyLoss()\n",
    "\n",
    "# ------- 3) SWA wrapper -------\n",
    "swa_model = AveragedModel(model)\n",
    "swa_start = int(EPOCHS_F * 0.6)\n",
    "swa_sched = SWALR(opt, swa_lr=LR_BASE * 0.5)\n",
    "best_vA = 0.0\n",
    "\n",
    "# ------- 4) Fine-tuning of loop -------\n",
    "for ep in range(1, EPOCHS_F + 1):\n",
    "    # ------- Training pass ------- \n",
    "    model.train()\n",
    "    tr_tot = 0.0\n",
    "    tr_cor = 0.0\n",
    "    tr_loss = 0.0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        opt.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = ce(out, yb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "\n",
    "        tr_loss += loss.item() * yb.size(0)\n",
    "        tr_cor += (out.argmax(1) == yb).sum().item()\n",
    "        tr_tot += yb.size(0)\n",
    "\n",
    "    trL = tr_loss / tr_tot\n",
    "    trA = tr_cor / tr_tot\n",
    "\n",
    "    # ------- Validation pass ------- \n",
    "    model.eval()\n",
    "    vl_tot = vl_cor = vl_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            out = model(xb)\n",
    "            vl_loss += ce(out, yb).item() * yb.size(0)\n",
    "            vl_cor += (out.argmax(1) == yb).sum().item()\n",
    "            vl_tot += yb.size(0)\n",
    "\n",
    "    vL = vl_loss / vl_tot\n",
    "    vA = vl_cor / vl_tot\n",
    "\n",
    "    # ------- Recording & logging ------- \n",
    "    tr_losses.append(trL)\n",
    "    tr_accs.append( trA)\n",
    "    vl_losses.append(vL)\n",
    "    vl_accs.append( vA)\n",
    "\n",
    "    lr_now = sched.get_last_lr()[0]\n",
    "    print(f\"fine-tune E{ep:02} | LR {lr_now:.2e}; Tr {trL:.3f}/{trA:.3f}; V {vL:.3f}/{vA:.3f}\")\n",
    "\n",
    "    # ------ SWA updating & best-acc tracking ------- \n",
    "    if ep >= swa_start:\n",
    "        swa_model.update_parameters(model)\n",
    "        swa_sched.step()\n",
    "    best_vA = max(best_vA, vA)\n",
    "\n",
    "print(f\"best val-acc before SWA {best_vA:.3f}\")\n",
    "\n",
    "# ------ 5) Finalizing SWA -------\n",
    "update_bn(\n",
    "    train_loader,\n",
    "    swa_model,\n",
    "    device=device\n",
    ")\n",
    "torch.save(\n",
    "    swa_model.state_dict(),\n",
    "    f\"{CHECKPOINT_ROOT}/stage3_mixup_swa_final.pt\"\n",
    ")\n",
    "print(\"SWA model saved → stage3_mixup_swa_final.pt\")\n",
    "\n",
    "# ------ 6) Plotting train & val curves ------\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(tr_losses, label='train loss')\n",
    "plt.plot(vl_losses, label='val loss')\n",
    "plt.title(\"Stage 3 Loss\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(tr_accs, label='train acc')\n",
    "plt.plot(vl_accs, label='val acc')\n",
    "plt.title(\"Stage 3 Accuracy\")\n",
    "plt.xlabel(\"Num of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb718d-7e89-46ac-9c74-b076094e9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking device\n",
    "device = torch.device(\n",
    "    \"mps\" if torch.backends.mps.is_built()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# ------- Rebuilding the neural network with same head / dropout -------\n",
    "NUM_CLASSES = 9\n",
    "model = models.resnet152(pretrained=False)\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Linear(model.fc.in_features, 512),\n",
    "    nn.GELU(),\n",
    "    nn.Dropout(0.10), # 10% head‐dropout used in fine‐tuning\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "# ------ Loading SWA weights directly onto device -------\n",
    "ckpt = torch.load(\n",
    "    f\"{CHECKPOINT_ROOT}/stage3_mixup_swa_final.pt\",\n",
    "    map_location=device      # loading straight onto MPS / CUDA\n",
    ")\n",
    "model.load_state_dict(ckpt, strict=False)\n",
    "\n",
    "# Moving model to MPS / CUDA\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ------- Single CE‐LS (eps = 0.005) loss fn -------\n",
    "def ce_ls(logits, targets, eps=0.005):\n",
    "    logp = logits.log_softmax(1)\n",
    "    nll = F.nll_loss(logp, targets, reduction='none')\n",
    "    return (1 - eps) * nll.mean() + eps * (-logp.mean(1)).mean()\n",
    "\n",
    "# ------- Looping over val set --------\n",
    "tot = 0, \n",
    "cor = 0\n",
    "vloss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        out = model(xb)\n",
    "        vloss += ce_ls(out, yb).item() * yb.size(0)\n",
    "        cor += (out.argmax(1) == yb).sum().item()\n",
    "        tot += yb.size(0)\n",
    "\n",
    "print(f\"SWA: val-loss {vloss/tot:.4f}; val-acc {cor/tot:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33500669-88b8-4955-a289-2dc909c9842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scripted = torch.jit.script(model.cpu())\n",
    "scripted.save(f\"{CHECKPOINT_ROOT}/wafer_resnet152_swa.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313ec94f-b9d0-42ed-8733-6c2a7f1f4c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.jit.load(f\"{CHECKPOINT_ROOT}/wafer_resnet152_swa.ptl\").to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7e3cc4-cea9-4222-9b0f-486ed3d712e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Model-info helper -------\n",
    "\n",
    "def model_stats(model: torch.nn.Module,\n",
    "                input_size=(3, 128, 128),\n",
    "                show_summary=False):\n",
    "    \"\"\"\n",
    "    Prints model name, total / trainable parameter counts, a per-block\n",
    "    breakdown, and (optionally) a torchsummary table\n",
    "    \"\"\"\n",
    "    def cnt(mod):\n",
    "        return sum(p.numel() for p in mod.parameters())\n",
    "\n",
    "    blocks = OrderedDict([\n",
    "        (\"conv1+bn1\", model.conv1),\n",
    "        (\"layer1\", model.layer1),\n",
    "        (\"layer2\", model.layer2),\n",
    "        (\"layer3\", model.layer3),\n",
    "        (\"layer4\", model.layer4),\n",
    "        (\"head (fc)\", model.fc),\n",
    "    ])\n",
    "\n",
    "    total_params = cnt(model)\n",
    "    trainable_params = sum(p.numel() for p in model.parameters()\n",
    "                           if p.requires_grad)\n",
    "\n",
    "    # ------- Header -------\n",
    "    print(\"=============================================================\")\n",
    "    print(f\"Model        : {model.__class__.__name__}\")\n",
    "    print(f\"Total params : {total_params:,}\")\n",
    "    print(f\"Trainable    : {trainable_params:,}\")\n",
    "    print(f\"Frozen       : {total_params - trainable_params:,}\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(f\"{\"Block\":<15} {\"Params\":>15} {\"Trainable\":>15}\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    for name, blk in blocks.items():\n",
    "        blk_tot  = cnt(blk)\n",
    "        blk_tr   = sum(p.numel() for p in blk.parameters()\n",
    "                       if p.requires_grad)\n",
    "        print(f\"{name:<15} {blk_tot:>15,} {blk_tr:>15,}\")\n",
    "    print(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "    # Optional full layer-by-layer table\n",
    "    if show_summary:\n",
    "        try:\n",
    "            from torchsummary import summary\n",
    "            summary(model, input_size,\n",
    "                    device=str(next(model.parameters()).device))\n",
    "        except ImportError:\n",
    "            print(\"Run pip install torchsummary for the detailed table.\")\n",
    "\n",
    "# ------ Usage (after model is built / loaded and on the correct device) -------\n",
    "model_stats(\n",
    "    model,\n",
    "    input_size=(3, 128, 128),\n",
    "    show_summary=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
